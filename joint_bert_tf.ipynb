{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "dataset = \"SNIPS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'bert-base-uncased'\n",
    "MAX_SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    dataset = []\n",
    "    with open(path) as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = load_data(os.path.join('data', dataset, 'train.json'))\n",
    "train_seq_in = []\n",
    "train_seq_out = []\n",
    "train_labels = []\n",
    "for element in train_raw:\n",
    "    train_seq_in.append(element['utterance'])\n",
    "    train_seq_out.append(element['slots'])\n",
    "    train_labels.append(element['intent'])\n",
    "\n",
    "# same for valid\n",
    "valid_raw = load_data(os.path.join('data', dataset, 'valid.json'))\n",
    "val_seq_in = []\n",
    "val_seq_out = []\n",
    "val_labels = []\n",
    "for element in valid_raw:\n",
    "    val_seq_in.append(element['utterance'])\n",
    "    val_seq_out.append(element['slots'])\n",
    "    val_labels.append(element['intent'])\n",
    "\n",
    "# same for test\n",
    "test_raw = load_data(os.path.join('data', dataset, 'test.json'))\n",
    "test_seq_in = []\n",
    "test_seq_out = []\n",
    "test_labels = []\n",
    "for element in test_raw:\n",
    "    test_seq_in.append(element['utterance'])\n",
    "    test_seq_out.append(element['slots'])\n",
    "    test_labels.append(element['intent'])\n",
    "\n",
    "columns_name = ['seq_in','seq_out','label']\n",
    "df_train = pd.DataFrame(list(zip(train_seq_in,train_seq_out,train_labels)), columns=columns_name)\n",
    "df_val = pd.DataFrame(list(zip(val_seq_in,val_seq_out,val_labels)), columns=columns_name)\n",
    "df_test = pd.DataFrame(list(zip(test_seq_in,test_seq_out,test_labels)),columns=columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_intent_num = len(df_train['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train['label'])\n",
    "\n",
    "train_labels_encoded = le.transform(df_train['label'].values)\n",
    "val_labels_encoded = le.transform(df_val['label'].values)\n",
    "test_labels_encoded = le.transform(df_test['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def encode_dataset(tokenizer, text_sequences, max_length):\n",
    "    token_ids = np.zeros(shape=(len(text_sequences), max_length),\n",
    "                         dtype=np.int32)\n",
    "    for i, text_sequence in enumerate(text_sequences):\n",
    "        encoded = tokenizer.encode(text_sequence)\n",
    "        token_ids[i, 0:len(encoded)] = encoded\n",
    "    attention_masks = (token_ids != 0).astype(np.int32)\n",
    "    return {\"input_ids\": token_ids, \"attention_masks\": attention_masks}\n",
    "\n",
    "\n",
    "train_bert_input = encode_dataset(tokenizer, df_train[\"seq_in\"], MAX_SEQ_LEN)\n",
    "valid_bert_input = encode_dataset(tokenizer, df_val[\"seq_in\"], MAX_SEQ_LEN)\n",
    "test_bert_input = encode_dataset(tokenizer, df_test[\"seq_in\"], MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tf_bert_model = TFBertModel.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(text_seq, label_seq, word_to_index, tokenizer):\n",
    "\n",
    "  encoded_seq_labels = []\n",
    "  extra_keys = 0\n",
    "  extra_key_names = []\n",
    "  \n",
    "  for i, (text, labels) in enumerate(zip(text_seq, label_seq)):\n",
    "    sent_level_label_encoding=[]\n",
    "    for word, word_label in zip(text.split(), labels.split()):\n",
    "      word_level_label_encoding = []\n",
    "      word_tokens = tokenizer.tokenize(word)\n",
    "      for w in word_tokens:\n",
    "        #handling when word has word level tokenizatation, and label start with B-\n",
    "        if w.startswith(\"#\") and word_label.startswith(\"B-\"):\n",
    "          word_label = word_label.replace(\"B-\",\"I-\")\n",
    "          if word_label not in word_to_index.keys():\n",
    "            extra_keys +=1\n",
    "            extra_key_names.append(word_label)\n",
    "            word_to_index[word_label] = len(word_to_index)\n",
    "        word_level_label_encoding.append(word_label) \n",
    "      sent_level_label_encoding.extend(word_level_label_encoding)\n",
    "    #assert to check weather we have same number of token and labels\n",
    "    assert(len(sent_level_label_encoding) == len(tokenizer.tokenize(text)))\n",
    "    encoded_seq_labels.append(\" \".join(sent_level_label_encoding))\n",
    "\n",
    "  print(\"Total number of keys added\\n\",extra_keys)\n",
    "  print(\"Extra keys are,\\n \", extra_key_names)\n",
    "  return encoded_seq_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "seq_out_tokenizer = Tokenizer(filters='!\"#$%&()*+,./:;<=>?@[\\\\]^`{|}~\\t\\n', oov_token=\"UNK\",lower=False)\n",
    "seq_out_tokenizer.fit_on_texts(df_train[\"seq_out\"].tolist())\n",
    "seq_out_word_to_index = seq_out_tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of keys added\n",
      " 3\n",
      "Extra keys are,\n",
      "  ['I-year', 'I-condition_description', 'I-condition_temperature']\n",
      "Total number of keys added\n",
      " 0\n",
      "Extra keys are,\n",
      "  []\n",
      "Total number of keys added\n",
      " 0\n",
      "Extra keys are,\n",
      "  []\n"
     ]
    }
   ],
   "source": [
    "train_slots_encoded = encode_tokens(df_train['seq_in'].tolist(), df_train['seq_out'].tolist(), seq_out_word_to_index, tokenizer)\n",
    "val_slots_encoded = encode_tokens(df_val['seq_in'].tolist(), df_val['seq_out'].tolist(), seq_out_word_to_index, tokenizer)\n",
    "test_slots_encoded = encode_tokens(df_test['seq_in'].tolist(), df_test['seq_out'].tolist(), seq_out_word_to_index, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_out_tokenizer = Tokenizer(oov_token=\"UNK\",lower=False,filters='!\"#$%&()*+,./:;<=>?@\\\\^`{|}~\\t\\n')\n",
    "seq_out_tokenizer.fit_on_texts(train_slots_encoded)\n",
    "seq_out_tokenizer.word_index['PAD'] = 0\n",
    "seq_out_tokenizer.index_word[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slots_tokenized = seq_out_tokenizer.texts_to_sequences(train_slots_encoded)\n",
    "val_slots_tokenized = seq_out_tokenizer.texts_to_sequences(val_slots_encoded)\n",
    "test_slots_tokenized = seq_out_tokenizer.texts_to_sequences(test_slots_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_slots_for_bert(tokenized_slots, max_len):\n",
    "\n",
    "  final = np.zeros(shape=(len(tokenized_slots),max_len), dtype='int32')\n",
    "\n",
    "  for i,slot in enumerate(tokenized_slots):\n",
    "    final[i, 1:len(slot)+1] = slot\n",
    "  return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slots = prepare_slots_for_bert(train_slots_tokenized, MAX_SEQ_LEN)\n",
    "valid_slots = prepare_slots_for_bert(val_slots_tokenized, MAX_SEQ_LEN)\n",
    "test_slots = prepare_slots_for_bert(test_slots_tokenized, MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def get_slots_with_pad(text_seq, maxlen):\n",
    "  final = np.full((len(text_seq), maxlen), \"PAD\", dtype='U32')\n",
    "\n",
    "  for i,text in enumerate(text_seq):\n",
    "    splitted_text = text.split()\n",
    "    final[i, 1:len(splitted_text)+1] = splitted_text\n",
    "  return final\n",
    "train_slots_encoed_with_pad_token = get_slots_with_pad(train_slots_encoded, MAX_SEQ_LEN)\n",
    "val_slots_encoded_with_pad_token =  get_slots_with_pad(val_slots_encoded, MAX_SEQ_LEN)\n",
    "test_slots_encoded_with_pad_token = get_slots_with_pad(test_slots_encoded, MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "import tensorflow as tf\n",
    "class JointIntentAndSlotFillingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, total_intent_no=None, total_slot_no=None,\n",
    "                 model_name=BERT_MODEL, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(model_name)\n",
    "        self.dropout = Dropout(dropout_prob)\n",
    "        self.intent_classifier = Dense(total_intent_no, activation='softmax')\n",
    "        self.slot_classifier = Dense(total_slot_no, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        bert_output = self.bert(inputs)\n",
    "\n",
    "        sequence_output = self.dropout(bert_output[0])\n",
    "        slots_predicted = self.slot_classifier(sequence_output)\n",
    "\n",
    "        pooled_output = self.dropout(bert_output[1])\n",
    "        intent_predicted = self.intent_classifier(pooled_output)\n",
    "\n",
    "        return slots_predicted, intent_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "joint_model = JointIntentAndSlotFillingModel(\n",
    "    total_intent_no=7, total_slot_no=77,dropout_prob=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "opt = Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "losses = [SparseCategoricalCrossentropy(from_logits=False),\n",
    "          SparseCategoricalCrossentropy(from_logits=False)]\n",
    "metrics = [SparseCategoricalAccuracy('accuracy')]\n",
    "\n",
    "joint_model.compile(optimizer=opt, loss=losses, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "from seqeval.metrics import classification_report\n",
    "import shutil\n",
    "import pickle\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path exist, clearing all files under model_path\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: 'joint_model/Tensorboard\\\\logs\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(model_path):\n\u001b[0;32m      6\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel path exist, clearing all files under model_path\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m   shutil\u001b[39m.\u001b[39;49mrmtree(model_path)\n\u001b[0;32m      8\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCreating model_path\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:749\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    748\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m \u001b[39mreturn\u001b[39;00m _rmtree_unsafe(path, onerror)\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:614\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    612\u001b[0m         onerror(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mislink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n\u001b[0;32m    613\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m     _rmtree_unsafe(fullname, onerror)\n\u001b[0;32m    615\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    616\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:614\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    612\u001b[0m         onerror(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mislink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n\u001b[0;32m    613\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m     _rmtree_unsafe(fullname, onerror)\n\u001b[0;32m    615\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    616\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:614\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    612\u001b[0m         onerror(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mislink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n\u001b[0;32m    613\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m     _rmtree_unsafe(fullname, onerror)\n\u001b[0;32m    615\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    616\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:623\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    621\u001b[0m     os\u001b[39m.\u001b[39mrmdir(path)\n\u001b[0;32m    622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m--> 623\u001b[0m     onerror(os\u001b[39m.\u001b[39;49mrmdir, path, sys\u001b[39m.\u001b[39;49mexc_info())\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:621\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    619\u001b[0m             onerror(os\u001b[39m.\u001b[39munlink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n\u001b[0;32m    620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m     os\u001b[39m.\u001b[39;49mrmdir(path)\n\u001b[0;32m    622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m     onerror(os\u001b[39m.\u001b[39mrmdir, path, sys\u001b[39m.\u001b[39mexc_info())\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: 'joint_model/Tensorboard\\\\logs\\\\train'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "model_path = 'joint_model/'\n",
    "model_name = \"joint_model_weights_{val_loss:.2f}.ckpt\"\n",
    "if os.path.exists(model_path):\n",
    "  print(\"Model path exist, clearing all files under model_path\")\n",
    "  shutil.rmtree(model_path)\n",
    "else:\n",
    "  print(\"Creating model_path\")\n",
    "  os.makedirs(model_path)\n",
    "\n",
    "model_chk_point = ModelCheckpoint(filepath=os.path.join(model_path,model_name),monitor=\"val_loss\",save_best_only=True,save_weights_only=True)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\",min_delta=0.0001,patience=4,verbose=1)\n",
    "\n",
    "tensorboard_path = \"joint_model/Tensorboard/logs/\"\n",
    "if os.path.exists(tensorboard_path):\n",
    "  print(\"Tensorboard path exists, clearing all files under tensorboard_path\")\n",
    "  shutil.rmtree(tensorboard_path)\n",
    "else:\n",
    "  print(\"Creating tensorboard path\")\n",
    "  os.makedirs(tensorboard_path)\n",
    "\n",
    "tensorboard_cb = TensorBoard(log_dir=tensorboard_path)\n",
    "\n",
    "callback_list = [model_chk_point, tensorboard_cb, early_stopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = joint_model.fit(\n",
    "    train_bert_input, (train_slots,train_labels_encoded),\n",
    "    validation_data=(valid_bert_input, (valid_slots, val_labels_encoded)),\n",
    "    epochs=15, batch_size=128, callbacks=callback_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "760bf3b9c43a19e2fe1b4d509841bd16027d034122dfd0e7f3a29ff3458619a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
